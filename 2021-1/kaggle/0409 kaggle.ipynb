{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costa Rican Household Poverty Level Prediction \n",
    "(코스타리카 가구 빈곤 수준 예측)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM with random split for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# categorical로 encoding하는 함수 생성\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "    \n",
    "# decision tree의 feature importance plot을 그리는 함수 \n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_ # feature importance 입력\n",
    "    indices = np.argsort(importances)[::-1] # feature importance를 내림차순으로 정렬\n",
    "    \n",
    "    if display_results:\n",
    "        print(\"Feature ranking: \") #항상 True\n",
    "        \n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print('%d. feature %d (%f)'%(f+1, indices[f], importances[indices[f]])\n",
    "                  +'-'+ X_train.columns[indices[f]])\n",
    "        \n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "        \n",
    "        if importances[indices[f]] == 0.0: #importance가 0인경우\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "            \n",
    "        return ranked_list, zero_features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2')]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    #\n",
    "    for f_new, f1, f2 in feats_div: \n",
    "        df['fe_'+f_new] = (df[f1]/df[f2]).astype(np.float32)\n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_'+f_new] = (df[f1]-df[f2]).astype(np.float32)\n",
    "        \n",
    "    aggs_num = {'age': ['min', 'max', 'mean'],\n",
    "               'escolari': ['min', 'max', 'mean']}\n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    \n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "    \n",
    "    # houesehold의 합계\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]: \n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg'+name_+'_'+e[0]+\"_\" \n",
    "                                   + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "        \n",
    "    # Drop id\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? #~# 무슨 작업ㅠㅠ\n",
    "- query(조건식) = [조건식]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding된 것들을 label encoding으로 변환\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', \n",
    "               'elimbasu', 'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi', 'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "            \n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "            \n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        \n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy column 이름 추가\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # dataframe에 column을 더함\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "                    \n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deep=True(깊은 복사): 내부에 객체들까지 모두 새롭게 copy 되는 것\n",
    "- 얕은 복사: 슬라이싱을 통한 값 할당시 새로운 id 부여됨. 서로 영향을 받지 않음.\n",
    "\n",
    "/ 그 외 코드 무슨 작업ㅠㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/user/Desktop/학교/학회/21-1/kaggle2/train.csv')\n",
    "test = pd.read_csv('C:/Users/user/Desktop/학교/학회/21-1/kaggle2/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_) # encoding\n",
    "    return do_features(df_) # aggregate features 생성\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
    "\n",
    "# no인 경우 0으로 채움\n",
    "train.loc[train['edjefa'] =='no','edjefa'] =0\n",
    "train.loc[train['edjefe'] =='no','edjefe'] =0\n",
    "test.loc[test['edjefa'] =='no','edjefa'] =0\n",
    "test.loc[test['edjefe'] =='no','edjefe'] =0\n",
    "\n",
    "# yes인 경우 escolari로 채움\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1)\n",
    "          , \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1)\n",
    "          , \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1)\n",
    "         , \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1)\n",
    "         , \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes인 경우 4로 채움(gender, escolari 사이에 상호작용 존재 가능성 때문에)\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# int형태로 변환\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# max 값으로 지정\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
    "\n",
    "# na 값을 채움\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "# \n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OHE in techo is incomplete. A new column will be added before label encoding\n",
      "The OHE in instlevel is incomplete. A new column will be added before label encoding\n",
      "The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "#geo를 aggregate로 바꾸는 함수\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "# add some aggregates by geography\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age가 18 이상인 사람 수\n",
    "train['num_over_18'] = 0\n",
    "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = 0\n",
    "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "test['num_over_18'] = test['num_over_18'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? 코드 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 feature 생성\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog: household의 크기\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3: household의 총 인원\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] \n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent / people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent / people(under age 12)\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # 1인당 방의 수\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent / household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18'] # some households have no one over 18\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 없는 columns drop\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist()\n",
    "                 if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 함수\n",
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.2, seed=None):\n",
    "    train2 = train.copy()\n",
    "    \n",
    "    cv_hhs = np.random.choice(households, size=int(len(households)*test_percentage), replace=False)\n",
    "    \n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "    \n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "    \n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "# X = train.copy()\n",
    "\n",
    "# target variable 제거\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "train2 = X.copy()\n",
    "\n",
    "train_hhs = train2.idhogar\n",
    "\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n",
    "\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "# X, y train, test할당\n",
    "X_test = train2[cv_idx]\n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "# train on entire dataset\n",
    "X_train = train2\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the class weights for training with unbalanced classes\n",
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some features which aren't used by the LGBM or have very low importance\n",
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping을 위한 test subset 생성\n",
    "opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "\n",
    "# 6\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "# # 7\n",
    "# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1) \n",
    "\n",
    "fit_params={\"early_stopping_rounds\":500,\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'verbose': False,\n",
    "           }\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반드시 튜닝해야할 파라미터는  min_child_weight / max_depth / gamma\n",
    "- 'eta': GBM의 학습 속도와 유사\n",
    "- 'objective':'multi:softmax': softmax를 사용한 다중 클래스 분류, 예측된 클래스를 반환한다 \n",
    "- 'min_child_weight':  child의 관측에서 요구되는 최소 가중치의 합\n",
    "- 'num_class': 4, \n",
    "- 'colsample_bylevel': 트리의 레벨별로 훈련 데이터의 변수를 샘플링해주는 비율.\n",
    "- 'subsample': 각 트리마다의 관측 데이터 샘플링 비율 \n",
    "- 'colsample_bytree':트리를 생성할때 훈련 데이터에서 변수를 샘플링해주는 비율. \n",
    "- 'reg_lambda': 가중치에 대한 L2 정규화 용어\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "    \n",
    "    # early stopping할 test set을 얻기 위해 randomly split \n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "        \n",
    "    # 새롭게 분류된 데이터에 fit params로 업데이트\n",
    "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
    "    \n",
    "    # estimator 적용\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "    \n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    # reject some estimators based on their performance on train and test sets\n",
    "    if threshold:\n",
    "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "\n",
    "        # else recurse until we get a better one\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator\n",
    "    \n",
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- isinstance(a,b): a가 b형태인지?\n",
    "- ExtraTreesClassifier: ExtraTreesClassifier가 사용하는 결정 트리는 ExtraTreeClassifier \n",
    "- (RandomForestClassifier 클래스가 사용하는 결정 트리는 DecisionTreeClassifier )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:42:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:42:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30149\tvalidation_0-macroF1:0.62363\n",
      "[50]\tvalidation_0-mlogloss:0.92546\tvalidation_0-macroF1:0.59088\n",
      "[100]\tvalidation_0-mlogloss:0.91596\tvalidation_0-macroF1:0.58415\n",
      "[150]\tvalidation_0-mlogloss:0.91811\tvalidation_0-macroF1:0.59982\n",
      "[200]\tvalidation_0-mlogloss:0.91495\tvalidation_0-macroF1:0.58908\n",
      "[250]\tvalidation_0-mlogloss:0.91505\tvalidation_0-macroF1:0.58968\n",
      "[299]\tvalidation_0-mlogloss:0.91526\tvalidation_0-macroF1:0.60058\n",
      "Train F1: 0.9032991723706599\n",
      "Test F1: 0.43255609542659534\n",
      "[14:43:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:43:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30451\tvalidation_0-macroF1:0.65562\n",
      "[50]\tvalidation_0-mlogloss:0.92630\tvalidation_0-macroF1:0.60009\n",
      "[100]\tvalidation_0-mlogloss:0.92365\tvalidation_0-macroF1:0.61138\n",
      "[150]\tvalidation_0-mlogloss:0.91865\tvalidation_0-macroF1:0.59696\n",
      "[200]\tvalidation_0-mlogloss:0.91636\tvalidation_0-macroF1:0.59460\n",
      "[250]\tvalidation_0-mlogloss:0.91641\tvalidation_0-macroF1:0.59790\n",
      "[299]\tvalidation_0-mlogloss:0.91609\tvalidation_0-macroF1:0.59613\n",
      "Train F1: 0.9216074863843149\n",
      "Test F1: 0.4097880147373626\n",
      "[14:44:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:44:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29980\tvalidation_0-macroF1:0.61890\n",
      "[50]\tvalidation_0-mlogloss:0.94222\tvalidation_0-macroF1:0.53684\n",
      "[100]\tvalidation_0-mlogloss:0.94006\tvalidation_0-macroF1:0.53589\n",
      "[150]\tvalidation_0-mlogloss:0.93802\tvalidation_0-macroF1:0.53611\n",
      "[200]\tvalidation_0-mlogloss:0.93544\tvalidation_0-macroF1:0.53185\n",
      "[250]\tvalidation_0-mlogloss:0.93682\tvalidation_0-macroF1:0.54241\n",
      "[299]\tvalidation_0-mlogloss:0.93741\tvalidation_0-macroF1:0.54081\n",
      "Train F1: 0.9019173264713061\n",
      "Test F1: 0.47857873529398814\n",
      "[14:44:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:44:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.31053\tvalidation_0-macroF1:0.65356\n",
      "[50]\tvalidation_0-mlogloss:0.93044\tvalidation_0-macroF1:0.58369\n",
      "[100]\tvalidation_0-mlogloss:0.92956\tvalidation_0-macroF1:0.59360\n",
      "[150]\tvalidation_0-mlogloss:0.92805\tvalidation_0-macroF1:0.59476\n",
      "[200]\tvalidation_0-mlogloss:0.92781\tvalidation_0-macroF1:0.59742\n",
      "[250]\tvalidation_0-mlogloss:0.92771\tvalidation_0-macroF1:0.59669\n",
      "[299]\tvalidation_0-mlogloss:0.92746\tvalidation_0-macroF1:0.59388\n",
      "Train F1: 0.8936181455144405\n",
      "Test F1: 0.42676228470111455\n",
      "[14:45:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:45:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30272\tvalidation_0-macroF1:0.63004\n",
      "[50]\tvalidation_0-mlogloss:0.91995\tvalidation_0-macroF1:0.63026\n",
      "[100]\tvalidation_0-mlogloss:0.91341\tvalidation_0-macroF1:0.62019\n",
      "[150]\tvalidation_0-mlogloss:0.91144\tvalidation_0-macroF1:0.61877\n",
      "[200]\tvalidation_0-mlogloss:0.91097\tvalidation_0-macroF1:0.61611\n",
      "[250]\tvalidation_0-mlogloss:0.90940\tvalidation_0-macroF1:0.61330\n",
      "[299]\tvalidation_0-mlogloss:0.90984\tvalidation_0-macroF1:0.62504\n",
      "Train F1: 0.8537353909722818\n",
      "Test F1: 0.41962739688655915\n",
      "[14:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:46:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30508\tvalidation_0-macroF1:0.62627\n",
      "[50]\tvalidation_0-mlogloss:0.91256\tvalidation_0-macroF1:0.59578\n",
      "[100]\tvalidation_0-mlogloss:0.91324\tvalidation_0-macroF1:0.60375\n",
      "[150]\tvalidation_0-mlogloss:0.90993\tvalidation_0-macroF1:0.60343\n",
      "[200]\tvalidation_0-mlogloss:0.90886\tvalidation_0-macroF1:0.60702\n",
      "[250]\tvalidation_0-mlogloss:0.90673\tvalidation_0-macroF1:0.60589\n",
      "[299]\tvalidation_0-mlogloss:0.90551\tvalidation_0-macroF1:0.60875\n",
      "Train F1: 0.893278948113702\n",
      "Test F1: 0.40950522177474485\n",
      "[14:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:46:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30069\tvalidation_0-macroF1:0.61618\n",
      "[50]\tvalidation_0-mlogloss:0.94132\tvalidation_0-macroF1:0.60322\n",
      "[100]\tvalidation_0-mlogloss:0.93846\tvalidation_0-macroF1:0.62118\n",
      "[150]\tvalidation_0-mlogloss:0.93790\tvalidation_0-macroF1:0.60610\n",
      "[200]\tvalidation_0-mlogloss:0.93585\tvalidation_0-macroF1:0.61751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250]\tvalidation_0-mlogloss:0.93561\tvalidation_0-macroF1:0.61413\n",
      "[299]\tvalidation_0-mlogloss:0.93615\tvalidation_0-macroF1:0.60494\n",
      "Train F1: 0.9221972607612193\n",
      "Test F1: 0.3986409646719361\n",
      "[14:47:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:47:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29604\tvalidation_0-macroF1:0.59826\n",
      "[50]\tvalidation_0-mlogloss:0.87632\tvalidation_0-macroF1:0.54383\n",
      "[100]\tvalidation_0-mlogloss:0.87545\tvalidation_0-macroF1:0.54139\n",
      "[150]\tvalidation_0-mlogloss:0.87466\tvalidation_0-macroF1:0.52726\n",
      "[200]\tvalidation_0-mlogloss:0.87170\tvalidation_0-macroF1:0.52933\n",
      "[250]\tvalidation_0-mlogloss:0.87091\tvalidation_0-macroF1:0.53216\n",
      "[299]\tvalidation_0-mlogloss:0.87114\tvalidation_0-macroF1:0.53002\n",
      "Train F1: 0.9167958626332275\n",
      "Test F1: 0.4760572709292852\n",
      "[14:48:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:48:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29567\tvalidation_0-macroF1:0.65573\n",
      "[50]\tvalidation_0-mlogloss:0.90117\tvalidation_0-macroF1:0.61296\n",
      "[100]\tvalidation_0-mlogloss:0.89281\tvalidation_0-macroF1:0.62898\n",
      "[150]\tvalidation_0-mlogloss:0.89252\tvalidation_0-macroF1:0.62557\n",
      "[200]\tvalidation_0-mlogloss:0.89285\tvalidation_0-macroF1:0.62325\n",
      "[250]\tvalidation_0-mlogloss:0.89066\tvalidation_0-macroF1:0.62592\n",
      "[299]\tvalidation_0-mlogloss:0.89063\tvalidation_0-macroF1:0.62378\n",
      "Train F1: 0.897732808518849\n",
      "Test F1: 0.38912388295726263\n",
      "[14:49:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:49:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30338\tvalidation_0-macroF1:0.64177\n",
      "[50]\tvalidation_0-mlogloss:0.93219\tvalidation_0-macroF1:0.56776\n",
      "[100]\tvalidation_0-mlogloss:0.93085\tvalidation_0-macroF1:0.56145\n",
      "[150]\tvalidation_0-mlogloss:0.92958\tvalidation_0-macroF1:0.55817\n",
      "[200]\tvalidation_0-mlogloss:0.92855\tvalidation_0-macroF1:0.56430\n",
      "[250]\tvalidation_0-mlogloss:0.92748\tvalidation_0-macroF1:0.56268\n",
      "[299]\tvalidation_0-mlogloss:0.92823\tvalidation_0-macroF1:0.57168\n",
      "Train F1: 0.9152290238987113\n",
      "Test F1: 0.45191734363024477\n",
      "[14:49:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:49:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29585\tvalidation_0-macroF1:0.62750\n",
      "[50]\tvalidation_0-mlogloss:0.92193\tvalidation_0-macroF1:0.57602\n",
      "[100]\tvalidation_0-mlogloss:0.91886\tvalidation_0-macroF1:0.59356\n",
      "[150]\tvalidation_0-mlogloss:0.92071\tvalidation_0-macroF1:0.57990\n",
      "[200]\tvalidation_0-mlogloss:0.91971\tvalidation_0-macroF1:0.58448\n",
      "[250]\tvalidation_0-mlogloss:0.92083\tvalidation_0-macroF1:0.58209\n",
      "[299]\tvalidation_0-mlogloss:0.92197\tvalidation_0-macroF1:0.58574\n",
      "Train F1: 0.9086304154267907\n",
      "Test F1: 0.42506181624577444\n",
      "[14:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.29398\tvalidation_0-macroF1:0.60096\n",
      "[50]\tvalidation_0-mlogloss:0.90129\tvalidation_0-macroF1:0.60130\n",
      "[100]\tvalidation_0-mlogloss:0.89854\tvalidation_0-macroF1:0.60393\n",
      "[150]\tvalidation_0-mlogloss:0.89327\tvalidation_0-macroF1:0.60894\n",
      "[200]\tvalidation_0-mlogloss:0.89450\tvalidation_0-macroF1:0.60906\n",
      "[250]\tvalidation_0-mlogloss:0.89603\tvalidation_0-macroF1:0.61958\n",
      "[299]\tvalidation_0-mlogloss:0.89426\tvalidation_0-macroF1:0.61342\n",
      "Train F1: 0.8662550899727971\n",
      "Test F1: 0.4462982085773469\n",
      "[14:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:51:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30687\tvalidation_0-macroF1:0.63599\n",
      "[50]\tvalidation_0-mlogloss:0.92489\tvalidation_0-macroF1:0.61276\n",
      "[100]\tvalidation_0-mlogloss:0.91769\tvalidation_0-macroF1:0.60418\n",
      "[150]\tvalidation_0-mlogloss:0.91387\tvalidation_0-macroF1:0.59966\n",
      "[200]\tvalidation_0-mlogloss:0.91290\tvalidation_0-macroF1:0.60113\n",
      "[250]\tvalidation_0-mlogloss:0.91024\tvalidation_0-macroF1:0.60182\n",
      "[299]\tvalidation_0-mlogloss:0.90875\tvalidation_0-macroF1:0.59118\n",
      "Train F1: 0.8643489385507681\n",
      "Test F1: 0.4261661692217248\n",
      "[14:51:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:51:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.30393\tvalidation_0-macroF1:0.64111\n",
      "[50]\tvalidation_0-mlogloss:0.88754\tvalidation_0-macroF1:0.56993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalidation_0-mlogloss:0.88308\tvalidation_0-macroF1:0.57654\n",
      "[150]\tvalidation_0-mlogloss:0.88080\tvalidation_0-macroF1:0.57587\n",
      "[200]\tvalidation_0-mlogloss:0.88118\tvalidation_0-macroF1:0.57169\n",
      "[250]\tvalidation_0-mlogloss:0.87879\tvalidation_0-macroF1:0.56565\n",
      "[299]\tvalidation_0-mlogloss:0.87909\tvalidation_0-macroF1:0.55778\n",
      "Train F1: 0.9260422104387851\n",
      "Test F1: 0.4422197158052913\n",
      "[14:52:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:52:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.31152\tvalidation_0-macroF1:0.66565\n",
      "[50]\tvalidation_0-mlogloss:0.93441\tvalidation_0-macroF1:0.61479\n",
      "[100]\tvalidation_0-mlogloss:0.93126\tvalidation_0-macroF1:0.61969\n",
      "[150]\tvalidation_0-mlogloss:0.92720\tvalidation_0-macroF1:0.60744\n",
      "[200]\tvalidation_0-mlogloss:0.92822\tvalidation_0-macroF1:0.60770\n",
      "[250]\tvalidation_0-mlogloss:0.92681\tvalidation_0-macroF1:0.61413\n",
      "[299]\tvalidation_0-mlogloss:0.92572\tvalidation_0-macroF1:0.61390\n",
      "Train F1: 0.8921481382338412\n",
      "Test F1: 0.4064181283186934\n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "# learning rate 감소를 이용하여 최종 모델 train\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train F1: 0.8921481382338412\n",
    "- Test F1: 0.4064181283186934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a single LGBM Classifier: 0.8182\n",
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8936\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8972\n"
     ]
    }
   ],
   "source": [
    "# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'soft'\n",
    "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "vc.voting = 'hard'\n",
    "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Validation score of a single LGBM Classifier: 0.8182\n",
    "- Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8936\n",
    "- Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8972\n",
    "\n",
    "-> 하드 보팅을 이용한 lightGBM의 validation score가 가장 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어느 모델에도 쓰이지 않는 features 확인\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? 결과가 안나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking: \n",
      "1. feature 59 (0.019646)-agg18_escolari_MAX\n"
     ]
    }
   ],
   "source": [
    "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? 결과가 다르게 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
    "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
    "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
    "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
    "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
    "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
    "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
    "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
    "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
    "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
    "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
    "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
    "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
    "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
    "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
    "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
    "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
    "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
    "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
    "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
    "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
    "\n",
    "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
    "       'fe_tablet_adult_density', 'fe_tablet_density'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1: 0.8922809921044023\n",
      "Test F1: 0.39897901682832504\n",
      "Train F1: 0.8922765415997481\n",
      "Test F1: 0.39528370858640194\n",
      "Train F1: 0.8926874302367859\n",
      "Test F1: 0.42090364057006613\n",
      "Train F1: 0.8955032250142193\n",
      "Test F1: 0.4659504148962913\n",
      "Train F1: 0.8922219399299893\n",
      "Test F1: 0.4320212468481338\n",
      "Train F1: 0.8873324355646326\n",
      "Test F1: 0.4142471731899002\n",
      "Train F1: 0.8950923247523142\n",
      "Test F1: 0.46524015901720317\n",
      "Train F1: 0.9014403938961943\n",
      "Test F1: 0.43039397833717463\n",
      "Train F1: 0.8962961674308901\n",
      "Test F1: 0.4362494377793077\n",
      "Train F1: 0.8992669977533156\n",
      "Test F1: 0.406936886929147\n"
     ]
    }
   ],
   "source": [
    "# do the same thing for some extra trees classifiers\n",
    "ets = []    \n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
    "    ets.append(('rf{}'.format(i), rf))   \n",
    "\n",
    "vc2 = VotingClassifierLGBM(ets, voting='soft')    \n",
    "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8416\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# threshold를 통해 extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.8416\n",
      "Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# threshold 없이 extra drop cols\n",
    "vc2.voting = 'soft'\n",
    "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "vc2.voting = 'hard'\n",
    "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
    "\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
    "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? 무슨 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are not used by ANY models\n",
    "useless_features = []\n",
    "drop_features = set()\n",
    "counter = 0\n",
    "for est in vc2.estimators_:\n",
    "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
    "    useless_features.append(unused_features)\n",
    "    if counter == 0:\n",
    "        drop_features = set(unused_features)\n",
    "    else:\n",
    "        drop_features = drop_features.intersection(set(unused_features))\n",
    "    counter += 1\n",
    "    \n",
    "drop_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위랑 똑같이 안나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_voters(data, weights=[0.5, 0.5]):\n",
    "    # do soft voting with both classifiers\n",
    "    vc.voting=\"soft\"\n",
    "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
    "    vc2.voting=\"soft\"\n",
    "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
    "    \n",
    "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
    "    predictions = np.argmax(final_vote, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8856110917239444"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8803259911371839"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
    "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899860433381532"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
    "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
    "global_combo_score_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1)) 비율이 높은 분류기가 결과 값이 제일 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_subm = pd.DataFrame()\n",
    "y_subm['Id'] = test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "y_subm_lgb = y_subm.copy(deep=True)\n",
    "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
    "\n",
    "vc2.voting = 'soft'\n",
    "y_subm_rf = y_subm.copy(deep=True)\n",
    "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
    "\n",
    "y_subm_ens = y_subm.copy(deep=True)\n",
    "y_subm_ens['Target'] = combine_voters(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
    "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
    "y_subm_ens.to_csv(sub_file_ens, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
